{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZsIXn4cYxDa",
    "outputId": "e227fac2-32fe-4e9c-eb46-a7fe0416d29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Collecting groq\n",
      "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h11, faiss-cpu, httpcore, httpx, groq, sentence-transformers\n",
      "Successfully installed faiss-cpu-1.9.0 groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 sentence-transformers-3.1.1\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Collecting openai\n",
      "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain)\n",
      "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.10->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.132-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: tenacity, orjson, jsonpointer, jiter, requests-toolbelt, jsonpatch, openai, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "Successfully installed jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.3 langchain-core-0.3.10 langchain-text-splitters-0.3.0 langsmith-0.1.132 openai-1.51.2 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.8)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.10)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.132)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (2.23.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
      "Downloading langchain_community-0.3.2-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.2 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.10)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.132)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.132)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.11\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy sentence-transformers faiss-cpu scikit-learn  groq\n",
    "# For Groq, follow their specific documentation or API guide.\n",
    "!pip install langchain sentence-transformers faiss-cpu pandas openai\n",
    "!pip install langchain-community\n",
    "!pip install langchain-community\n",
    "!pip install --upgrade langchain langchain-community\n",
    "!pip install PyPDF2\n",
    "!pip install pymupdf\n",
    "!pip install transformers # Install the missing langchain-community package# Install the missing langchain-community package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgXIzx6PZCgw"
   },
   "source": [
    "Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "96f004b34cd34f66a80874e90bb6dee6",
      "394d4e58a2724de19060f655ba56f392",
      "cd1e9d7794114380a1fd9fd262f68e4e",
      "a138b12a4d104b6aa324d0e61edfff9e",
      "c18b469565d44fe5b29518ce292f04ba",
      "acd7adacb750451b8d491c50ceeee1e6",
      "60589d7cab5741c898fd2a7fdc195b01",
      "9f2bf0c39b434636a68deb8dd9c72db1",
      "699f60160898498198fe03cbee3ee8c0",
      "a647c42848b24c2cbd896c0fddcc7500",
      "d152c950cfd44cc08d571bf2ee3bf10c",
      "ac5fdb5d48c24f65b9cbd75143aa80fd",
      "418001bb1c064d0e9eb619ddc69643f9",
      "83e3852573af4285ae5938ce27c5c621",
      "7e9c70b7556f46e1a1931ae8ac6f1887",
      "a1cadb84653c42349b3abc3e9a27551d",
      "66f2856e7d384c7fa67784040bab55dd",
      "fa620f3b26f7416892f8f69857009213",
      "c7dcd832e86449fbb3cb4f3ee7f402be",
      "27e775d268304decbdc1128b7d001119",
      "dc543f7ce19545f882176edf040abb3c",
      "8a93ce5a17774d57b1915d230bcb9722",
      "8816c158773d46ffb3dc43a5c6a47754",
      "1cfa3c72430a4aaf80e096f217b83bd2",
      "d003709801a943ea97f4229131d8bc59",
      "4089292c857d4d95ba271d9fff088fee",
      "25d944309b014c6a997f006e304e5cf1",
      "be7d73bf305242c09de9bebdc410eb73",
      "59a29406c1934d708f203c7e589f43af",
      "ea117ff2a04949ccb36a2c3ab02837bd",
      "0c439b037c38454d8681c95a6e2051e3",
      "1a851372d723440889dfa6cd6ed17326",
      "a4f8016a42c6476cb2d5f55db88e2820",
      "00f52781767448a58fd2bf2dead7309c",
      "1274904e0d3a440c9e68e405a5ee5eb1",
      "7c1033b04ff647c3b09e2aa229c1f025",
      "18c4569379be445b893fef5dddd6a7b8",
      "1da23baf6e24489789ba2e77d788c3d9",
      "938672bdaefd443fbcb93b414c37bfaa",
      "c07a9afa31924e65810aff5c452b9047",
      "460b5b8947124c57aa1547a98e2715e3",
      "c94ce478e94c47d4aa16ae39e1335f68",
      "2fcd5dac94a1454ebadc72a5d2544b1b",
      "736149deb1e24bbea85b205e97f91fcd",
      "5134ad84b5d74936ae556de7e4e299ae",
      "90abea5dcf514c55afeae5414780f3cb",
      "b4f98871f6dc4d38a22caf02b4e05a8f",
      "3712a28c81454cc0944f9857965bde93",
      "8c74c74ed5b24361bfde7dc20903510e",
      "e1d016a3ade9452e98be59cebd0b3ef6",
      "0231531fa9044d76a7d9d4ec5734a9e3",
      "4b65da31d1c84236afe11e8debfb044c",
      "a489135c4b3d4a728d8f005f1342c4e1",
      "75215ccb44c44313bacfcb0c40acfe83",
      "084a50135ce045c39c4e835d152f2ed5",
      "325ea48fe3ea4212b7dd2b5495b224e6",
      "6014757add5345cf9c35dd7f62bb3eab",
      "d9adc92a6a544c56b2f36a7e6c26eb7e",
      "81931d156b1a4fcfbec83bfcb2875867",
      "cb35b46346df4321bfb89f996363b405",
      "afa1b14da7f84ed9a8f484a3c01bf60f",
      "02d84453195141c98c5596d3a490d789",
      "d949e956fef74a56979e8851a9a05af8",
      "ab360a8e6b114693808c6b4e94cbe9c0",
      "9318c9953b284f9892fb9e80a74bc0b4",
      "d07e73d39e8e491a93ea87cbd1625e8a",
      "2732d54a1de94ac1a889cbfcf24f311c",
      "cef1c868d63d41b981f279163263e572",
      "2961ee57ac474a858f84433180923734",
      "8ae8b15396ac48258fadaf5a802bb8bf",
      "2162cb5c6ab241e19b57d1426cebdfac",
      "b31b7050064f4a308e66d30fc8338b7b",
      "5d4b8e441c09406ca2bf8ce9419df156",
      "8576d4a702cd4ec39a9c489b4620eace",
      "4d4b861b771a457aadb77bd903e23931",
      "b61704df9388492a8092ece83912cdf4",
      "4650f0a2699042fb9568933abd0ca567",
      "3422ea8b0467497b80dbb04511c5561d",
      "872eaa7da36f4bafacc91344a9d3eb26",
      "bf0380bd7c304e71bd072acfa4269f3d",
      "2e730a1ed6024559b6d6e2ec8b245b8f",
      "d5568ffe0f3a46459460a283ff88bb3b",
      "80f2164d328b4dc8a2435cb795609d21",
      "3c24ab0ab3f54f08914de051ec3ee950",
      "96bdce4826b04c97b64358fd1d98a5a8",
      "807b7afa51b54b66bf2cf51747b5e9e6",
      "75a8df0904af40f7a4c9abf0a940131d",
      "ee73731cd4dc40e5b79c74cdca87cce8",
      "1846f483b6634c33be6812c0d6a34bb7",
      "05c2486d4019490eaad5d19d532b2542",
      "a0c7547e277e4ea5a1acfabef255d085",
      "a10f7a95012d49ee8e9c9e219746196a",
      "36cd8872426646ccb7b1349648e19c8c",
      "1d3b97a9101042579a822fce5c8ac5fb",
      "0b1b9d88fa544d038122c0d9d7569d73",
      "0454b5c86f7c408cb658d523f7cd6eb1",
      "fa0a94a6be4948cea86f4e9d38d394e6",
      "4776318629e545fbb8a4050d50dbcff6",
      "eaf41e7ae1fe4fca94e68e2a5bce5f44",
      "92a3d5f4fad94c7d8469bf9b32afc954",
      "435dd0bd72d64a2eafa0ef1dc6078adf",
      "70c5b496d9364ddfadc6603762d64588",
      "aae2a888652c4379a4c198f1941fe2b6",
      "88c981cdc6b04218909b55bb1d1b46e8",
      "87f13b38fba442bdbcfc3e6eff9a9110",
      "6c41ede9d98f477aba12cad09404efc2",
      "6c9baea02f234e09b8f9521a56e59474",
      "79c31c119dea4fa497b53a97d7cc794d",
      "519cbd524972481ab76d4e68c718ac93",
      "f3d63eca74ba4d8e8fac951c937b4a39",
      "98640bdb3a4a4973a05ec1eec79e14df",
      "6dfd3653b6e74b81bb579dad5a605765",
      "be4f9b89b8ad42d3b10cde504f7868a3",
      "cddc8bf573994d7b92be2fcb6f1f9740",
      "5712037751414c4dbfa5001ceb8e7baf",
      "1f678e2ac88e44c0bbe31578cf74e4ac",
      "1c05f1e015494715a3a2515e24004a0d",
      "ee3bad5de67f4689bb038b92db0a943a",
      "5786d4a486414b0591e8137dba5ad1eb",
      "1c5679c4beaa4cedb6ddfb047e29f86e",
      "f7abc49c4134452c89e07aad8635f3c8"
     ]
    },
    "id": "hJOI0eeoY2mO",
    "outputId": "412665e9-8685-4935-8315-7870d321f05a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f004b34cd34f66a80874e90bb6dee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5fdb5d48c24f65b9cbd75143aa80fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8816c158773d46ffb3dc43a5c6a47754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f52781767448a58fd2bf2dead7309c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5134ad84b5d74936ae556de7e4e299ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325ea48fe3ea4212b7dd2b5495b224e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2732d54a1de94ac1a889cbfcf24f311c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3422ea8b0467497b80dbb04511c5561d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1846f483b6634c33be6812c0d6a34bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a3d5f4fad94c7d8469bf9b32afc954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98640bdb3a4a4973a05ec1eec79e14df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-7a8915777441>:31: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')  # Corrected here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query or problem: network router is not configurating \n",
      "Refined query: Here is a refined version of the query for better search accuracy:\n",
      "\n",
      "\"Network router configuration not working or not applying changes\"\n",
      "\n",
      "I refined the query by:\n",
      "\n",
      "* Breaking the phrase \"not configurating\" into two words \"not\" and \"configuring\" to make the query more specific and searchable.\n",
      "* Changing \"is not\" to \"not\" to make the query more concise.\n",
      "* Adding the phrase \"not applying changes\" to make the query more specific about the issue, increasing the chances of finding\n",
      "Final Answer:\n",
      "Based on the retrieved context, I'd be happy to help you troubleshoot the issue with your network router configuration not working or not applying changes.\n",
      "\n",
      "Given the complexity of network configurations, it's essential to methodically approach the problem to identify the root cause. Here's a step-by-step approach to help you resolve the issue:\n",
      "\n",
      "1. **Check routing tables**: Verify that the routing tables on the router are correctly configured. A misconfigured routing table can cause the issue. Ensure that the routes are properly summarized, and there are no duplicate routes.\n",
      "\n",
      " Context relevance: Problem Chunk: Misconfigured Routing Tables\n",
      "\n",
      "2. **Inspect router or switch logs**: Review the logs of the router or switch to identify any error messages or anomalies that could indicate a malfunctioning device. This might help you pinpoint the issue.\n",
      "\n",
      " Context relevance: Problem Chunk: Malfunctioning Routers or Switches\n",
      "\n",
      "3. **Verify device compatibility**: Ensure that the router and other network devices are compatible with the network standards (e.g., TCP/IP, DNS, DHCP). Incompatibility can cause configuration issues.\n",
      "\n",
      " Context relevance: Problem Chunk: Device Incompatibility with Network Standards\n",
      "\n",
      "4. ** Optimize network speeds**: Check the network speeds to ensure they are not slow, which might be causing issues with the configuration not applying. Update your network infrastructure or optimize network settings for better performance.\n",
      "\n",
      " Context relevance: Problem Chunk: Slow Network Speeds\n",
      "\n",
      "5. **Consult network documentation**: Review the network documentation to ensure that the configuration changes are being applied correctly. Verify that the documentation is up-to-date and accurate.\n",
      "\n",
      " Context relevance: Problem Chunk: Poor Network Documentation\n",
      "\n",
      "6. **Check for configuration syntax errors**: Review the configuration files for any syntax errors. Ensure that the configuration files are well-formatted and free of errors.\n",
      "\n",
      "7. **Reset network configuration**: If the issue persists, try resetting the network configuration to its default state. This might help resolve any configuration conflicts or errors.\n",
      "\n",
      "8. **Consult with network experts**: If none of the above steps resolve the issue, it's recommended to consult with network experts or reach out to the network equipment manufacturers' support teams for further assistance.\n",
      "\n",
      "By following these steps, you should be able to identify and resolve the issue with your network router configuration not working or not applying changes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from groq import Groq\n",
    "\n",
    "# Load your dataset from Excel\n",
    "file_path = '/content/Data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Trim whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Extract problems and solutions\n",
    "try:\n",
    "    problems = df['Problem'].tolist()\n",
    "    solutions = df['Solution'].tolist()\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e} - Please check the column names.\")\n",
    "    exit()  # Exit if there is a KeyError\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding the problems\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')  # Corrected here\n",
    "\n",
    "# Create a DataFrameLoader - Remove the data argument\n",
    "# The text_column argument is available in newer versions of LangChain\n",
    "loader = DataFrameLoader(df, page_content_column='Problem') # Use page_content_column instead of text_column\n",
    "\n",
    "# Split documents into chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "# Load documents and then split them using the splitter\n",
    "documents = loader.load()\n",
    "documents = splitter.split_documents(documents)\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following problem description for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Example query to test the index\n",
    "query_text = input(\"Enter your query or problem: \")\n",
    "\n",
    "# Use Groq to refine the query\n",
    "refined_query_text = refine_query_with_groq(query_text)\n",
    "print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "# Perform the search for the top k most similar items\n",
    "retrieved_documents = vector_store.similarity_search(refined_query_text, k=5)\n",
    "\n",
    "# Combine retrieved documents as context for the generative model\n",
    "retrieval_context = \"\"\n",
    "for doc in retrieved_documents:\n",
    "    retrieval_context += f\"Problem Chunk: {doc.page_content}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that generates answers to user queries based on retrieved context.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context, provide a detailed solution to the query: {retrieval_context}\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",  # Assuming you're using LLaMA 3\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "# Generate the final answer with RAG\n",
    "final_answer = generate_answer_with_rag(refined_query_text, retrieval_context)\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKUsdvvXZJGU"
   },
   "source": [
    "Fixed chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9Kz95PqZLnA",
    "outputId": "4d58dd4e-8861-4290-e993-489d27f813dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query or problem: network router is not configurating \n",
      "Refined query: Here's a refined version of the query:\n",
      "\n",
      "\"Troubleshooting network router configuration issues\"\n",
      "\n",
      "This refined query is more concise, specific, and accurate. I replaced \"not configurating\" with \"configuration issues\" to better convey the problem, and added \"troubleshooting\" to indicate that you're looking for solutions or advice on how to resolve the issue. This should help improve search accuracy and yield more relevant results.\n",
      "Final Answer:\n",
      "Based on the retrieved context, I'll provide a detailed solution to troubleshoot network router configuration issues.\n",
      "\n",
      "**Step 1: Verify Network Connectivity**\n",
      "\n",
      "Before diving into router configuration, ensure that your network devices are connected properly and functioning correctly. Check the following:\n",
      "\n",
      "* Physical connections: Ensure that all cables are securely plugged in and free from damage.\n",
      "* Power cycles: Restart devices such as routers, switches, and modems to rule out any temporary glitches.\n",
      "* LEDs: Verify that all LEDs on your devices are lit or blinking as expected.\n",
      "\n",
      "**Step 2: Review Router Configuration**\n",
      "\n",
      "Check the router's configuration using the web-based interface (if available) or the router's CLI (if enabled). Look for any unusual or suspicious settings that might be causing issues. Pay attention to:\n",
      "\n",
      "* IP addresses: Check the IP address assigned to the router and ensure it's consistent with your network configuration.\n",
      "* Subnet mask: Verify that the subnet mask is set correctly to match your network topology.\n",
      "* Default gateway: Ensure that the default gateway is set correctly to point to the router's IP address.\n",
      "* DNS settings: Check if the router's DNS settings are set to use the correct DNS servers.\n",
      "\n",
      "**Step 3: Investigate Misconfigured Routing Tables**\n",
      "\n",
      "In case of routing table issues, use the `route` command (on Windows) or `netstat -r` (on Linux/macOS) to display the routing table. Look for:\n",
      "\n",
      "* Unreachable destinations: Check if there are any unreachable destinations listed in the routing table.\n",
      "* Unreachable networks: Verify that the network you're trying to reach is listed in the routing table with a valid next hop.\n",
      "* Corrupted entries: Check for corrupted or outdated entries in the routing table that might be causing issues.\n",
      "\n",
      "**Step 4: Troubleshoot Malfunctioning Routers or Switches**\n",
      "\n",
      "If the router or switch is malfunctioning, try:\n",
      "\n",
      "* Power cycling the device: Restart the device to rule out any temporary issues.\n",
      "* Flushing the ARP cache: Clear the ARP cache (Address Resolution Protocol) to resolve any ARP-related issues.\n",
      "* Repairing firmware: Check for firmware updates and repair or reinstall the firmware if necessary.\n",
      "\n",
      "**Step 5: Check Poor Network Documentation**\n",
      "\n",
      "If you're using a third-party router or switch, ensure that you have a clear understanding of its configuration and documentation. Look for any documentation that might provide insights into the router's settings and how to troubleshoot issues.\n",
      "\n",
      "**Step 6: Resolve DNS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from groq import Groq\n",
    "\n",
    "# Load your dataset from Excel\n",
    "file_path = '/content/Data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Trim whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Extract problems and solutions\n",
    "try:\n",
    "    problems = df['Problem'].tolist()\n",
    "    solutions = df['Solution'].tolist()\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e} - Please check the column names.\")\n",
    "    exit()  # Exit if there is a KeyError\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding the problems\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')  # Corrected here\n",
    "\n",
    "# Create a DataFrameLoader\n",
    "loader = DataFrameLoader(df, page_content_column='Problem')  # Use page_content_column instead of text_column\n",
    "\n",
    "# Split documents into fixed-size chunks\n",
    "chunk_size = 100  # Set your fixed chunk size\n",
    "splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)  # Set overlap to 0 for fixed chunking\n",
    "\n",
    "# Load documents and then split them using the splitter\n",
    "documents = loader.load()\n",
    "documents = splitter.split_documents(documents)\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following problem description for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Example query to test the index\n",
    "query_text = input(\"Enter your query or problem: \")\n",
    "\n",
    "# Use Groq to refine the query\n",
    "refined_query_text = refine_query_with_groq(query_text)\n",
    "print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "# Perform the search for the top k most similar items\n",
    "retrieved_documents = vector_store.similarity_search(refined_query_text, k=5)\n",
    "\n",
    "# Combine retrieved documents as context for the generative model\n",
    "retrieval_context = \"\"\n",
    "for doc in retrieved_documents:\n",
    "    retrieval_context += f\"Problem Chunk: {doc.page_content}\\n\"\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that generates answers to user queries based on retrieved context.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context, provide a detailed solution to the query: {retrieval_context}\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",  # Assuming you're using LLaMA 3\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "# Generate the final answer with RAG\n",
    "final_answer = generate_answer_with_rag(refined_query_text, retrieval_context)\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KljjvdQ0bFNZ"
   },
   "source": [
    "Recursive chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25B--PlCbII8",
    "outputId": "554acbcb-42b7-4fb2-ec64-548670a67e5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query or problem: network router is not configurating \n",
      "Refined query: Here's a refined version of the query:\n",
      "\n",
      "\"Router configuration issues: unable to configure or connect to a network router\"\n",
      "\n",
      "This refined query includes:\n",
      "\n",
      "* Specific keywords: \"Router configuration\" and \"issues\"\n",
      "* Clarified intent: The user is seeking help with configuring a network router, which is not functioning as expected\n",
      "* Additional context: \"unable to configure or connect to a network router\" provides more details about the issue and its scope\n",
      "Final Answer:\n",
      "Based on the retrieved context and the refined query, I'll provide a detailed solution to help the user resolve the issue with configuring or connecting to a network router.\n",
      "\n",
      "**Possible Causes:**\n",
      "\n",
      "Based on the context, the issue could be caused by one or a combination of the following factors:\n",
      "\n",
      "1. Misconfigured Routing Tables: The router's routing tables might be misconfigured, preventing the router from properly communicating with other devices on the network.\n",
      "2. Malfunctioning Routers or Switches: The router or switch itself might be malfunctioning, causing issues with connectivity and configuration.\n",
      "3. Poor Network Documentation: Inadequate or incomplete network documentation could make it challenging to identify and troubleshoot configuration issues.\n",
      "4. Device Incompatibility with Network Standards: The router or devices connected to it might not be compatible with the network standards, leading to configuration issues.\n",
      "5. Routing Protocol Conflicts: Conflicting routing protocols or configuration settings could be preventing the router from functioning correctly.\n",
      "\n",
      "**Troubleshooting Steps:**\n",
      "\n",
      "To resolve the issue, follow these troubleshooting steps:\n",
      "\n",
      "1. **Check Network Documentation**: Review your network documentation to ensure you have all the necessary information about your router, switches, and connected devices. Verify that the documentation is up-to-date and accurate.\n",
      "2. **Reset Router and Switches**: Reset the router and switches to their default settings. This will help eliminate any potential configuration issues and give you a clean slate to work from.\n",
      "3. **Verify Device Compatibility**: Ensure that all devices connected to the router are compatible with the network standards (e.g., TCP/IP, DNS, DHCP). Check the device manufacturer's documentation for compatibility information.\n",
      "4. **Check Routing Tables**: Review the router's routing tables to ensure they are properly configured. You can do this by logging into the router's web interface and checking the routing table settings.\n",
      "5. **Run a Diagnostic Test**: Run a diagnostic test on the router to identify any potential issues. This might help you identify the root cause of the problem.\n",
      "6. **Check for Conflicting Settings**: Verify that there are no conflicting settings between the router, switch, and devices connected to it. Check for any duplicate IP addresses, conflicting subnet masks, or conflicting DNS settings.\n",
      "7. **Consult Router Documentation**: Consult your router's documentation to ensure that you are using the correct configuration settings and guidelines.\n",
      "\n",
      "**Resolution:**\n",
      "\n",
      "To resolve the issue, you may need to:\n",
      "\n",
      "1. Reconfigure the router's routing tables to ensure proper communication with other\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from groq import Groq\n",
    "\n",
    "# Load your dataset from Excel\n",
    "file_path = '/content/Data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Trim whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Extract problems and solutions\n",
    "try:\n",
    "    problems = df['Problem'].tolist()\n",
    "    solutions = df['Solution'].tolist()\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e} - Please check the column names.\")\n",
    "    exit()  # Exit if there is a KeyError\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding the problems\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')  # Corrected here\n",
    "\n",
    "# Create a DataFrameLoader\n",
    "loader = DataFrameLoader(df, page_content_column='Problem')  # Use page_content_column instead of text_column\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter with desired chunk size and overlap\n",
    "max_chunk_size = 200  # Maximum size of each chunk\n",
    "# min_chunk_size = 100  # Minimum size of chunks - Removed this line\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=max_chunk_size,  - This line was modified\n",
    "    chunk_size = min(max_chunk_size, 1000), # chunk_size has a limit of 1000\n",
    "    chunk_overlap=20,\n",
    "    # min_chunk_size=min_chunk_size, - This line was removed\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Load documents and then split them using the splitter\n",
    "documents = loader.load()\n",
    "documents = splitter.split_documents(documents)\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following problem description for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Example query to test the index\n",
    "query_text = input(\"Enter your query or problem: \")\n",
    "\n",
    "# Use Groq to refine the query\n",
    "refined_query_text = refine_query_with_groq(query_text)\n",
    "print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "# Perform the search for the top k most similar items\n",
    "retrieved_documents = vector_store.similarity_search(refined_query_text, k=5)\n",
    "\n",
    "# Combine retrieved documents as context for the generative model\n",
    "retrieval_context = \"\"\n",
    "for doc in retrieved_documents:\n",
    "    retrieval_context += f\"Problem Chunk: {doc.page_content}\\n\"\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that generates answers to user queries based on retrieved context.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context, provide a detailed solution to the query: {retrieval_context}\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",  # Assuming you're using LLaMA 3\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "# Generate the final answer with RAG\n",
    "final_answer = generate_answer_with_rag(refined_query_text, retrieval_context)\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtyhglEhbcQb"
   },
   "source": [
    "Documentation chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jnxw5keBcTMO"
   },
   "outputs": [],
   "source": [
    "!pip install langchain>=0.0.260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HL_lz02Rbf_M",
    "outputId": "23d4010c-6a99-47dc-ae69-bc739f000baf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query or problem: network router is not configurating \n",
      "Refined query: Refined Query: \n",
      "\n",
      "\"Router configuration not working or not saving, troubleshooting and solutions\"\n",
      "Final Answer:\n",
      "Based on the retrieved context, there are several possible causes for router configuration not working or not saving. Here are some potential solutions and troubleshooting steps you can follow:\n",
      "\n",
      "1. **Malfunctioning Routers or Switches**: If the router is malfunctioning or faulty, it may be causing issues with the configuration not saving. Try restarting or resetting the router to its default settings. If the issue persists, it may be necessary to replace the router or switch.\n",
      "\n",
      "   Key points to check:\n",
      "   - Reboot or reset devices\n",
      "   - Check connections\n",
      "\n",
      "2. **DNS Configuration Issues**: If the DNS configuration is incorrect, it may cause issues with the router configuration not saving. Verify that the DNS server settings are correct, and ensure that the forwarders and root hints are properly configured.\n",
      "\n",
      "   Key points to check:\n",
      "   - Verify DNS server settings\n",
      "   - Ensure correct forwarders and root hints are configured\n",
      "\n",
      "3. **DNS Resolution Issues**: If there are DNS resolution issues, it may prevent the router configuration from saving. Try flushing the DNS cache, changing the DNS servers to Google (8.8.8.8) or Cloudflare (1.1.1.1), and restarting the router.\n",
      "\n",
      "   Key points to check:\n",
      "   - Flush DNS cache\n",
      "   - Change DNS servers to Google (8.8.8.8) or Cloudflare (1.1.1.1)\n",
      "   - Restart the router\n",
      "\n",
      "4. **Load Balancer Configuration Issues**: If the load balancer is not configured correctly, it may cause issues with the router configuration not saving. Review the load balancer settings to ensure that the health checks and distribution algorithms are properly configured.\n",
      "\n",
      "   Key points to check:\n",
      "   - Review load balancer settings\n",
      "   - Ensure proper health checks and distribution algorithms\n",
      "\n",
      "5. **Misconfigured Routing Tables**: If the routing tables are misconfigured, it may prevent the router configuration from saving. Check and correct the routing table entries, and ensure proper route propagation.\n",
      "\n",
      "   Key points to check:\n",
      "   - Check and correct routing table entries\n",
      "   - Ensure proper route propagation\n",
      "\n",
      "By following these potential causes and solutions, you should be able to troubleshoot and resolve the issue with the router configuration not working or not saving. If none of these solutions work, it may be necessary to consult with a network administrator or a router manufacturer's support team for further assistance.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from groq import Groq\n",
    "\n",
    "# Load your dataset from Excel\n",
    "file_path = '/content/Data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Trim whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Extract problems and solutions\n",
    "try:\n",
    "    problems = df['Problem'].tolist()\n",
    "    solutions = df['Solution'].tolist()\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e} - Please check the column names.\")\n",
    "    exit()  # Exit if there is a KeyError\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding the problems\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')  # Corrected here\n",
    "\n",
    "# Create a DataFrameLoader\n",
    "loader = DataFrameLoader(df, page_content_column='Problem')  # Use page_content_column instead of text_column\n",
    "\n",
    "# Function to perform documentation chunking based on paragraph breaks\n",
    "def documentation_chunking(documents):\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        # Split document by newlines (paragraph breaks)\n",
    "        paragraphs = doc.page_content.split('\\n\\n')  # Assuming double newlines separate paragraphs\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.strip():  # Ignore empty paragraphs\n",
    "                chunks.append(paragraph.strip())\n",
    "    return chunks\n",
    "\n",
    "# Load documents\n",
    "documents = loader.load()\n",
    "# Apply documentation chunking\n",
    "document_chunks = documentation_chunking(documents)\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following problem description for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Example query to test the index\n",
    "query_text = input(\"Enter your query or problem: \")\n",
    "\n",
    "# Use Groq to refine the query\n",
    "refined_query_text = refine_query_with_groq(query_text)\n",
    "print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "# Perform the search for the top k most similar items\n",
    "retrieved_documents = vector_store.similarity_search(refined_query_text, k=5)\n",
    "\n",
    "# Combine retrieved documents as context for the generative model\n",
    "retrieval_context = \"\"\n",
    "for doc in retrieved_documents:\n",
    "    retrieval_context += f\"Problem Chunk: {doc}\\n\"  # Change this to handle string chunks\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that generates answers to user queries based on retrieved context.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context, provide a detailed solution to the query: {retrieval_context}\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",  # Assuming you're using LLaMA 3\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "# Generate the final answer with RAG\n",
    "final_answer = generate_answer_with_rag(refined_query_text, retrieval_context)\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez-gnfd6ggep"
   },
   "source": [
    "Agentic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F04kQtrYgj6r",
    "outputId": "9ced73cd-8ece-404d-db97-16557b690481"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query or problem: network router is not configurating \n",
      "Refined query: Here is a refined version of your query:\n",
      "\n",
      "\"Router configuration issue: Device not configuring properly or not connecting to network\"\n",
      "\n",
      "This refined query is more specific and accurate, making it easier for search engines to return relevant results.\n",
      "Final Answer:\n",
      "Based on the retrieved context, the query \"Router configuration issue: Device not configuring properly or not connecting to network\" can be matched with potential causes from all the provided problem chunks. Here's a detailed solution:\n",
      "\n",
      "From the provided problem chunks, the following potential causes are related to the query:\n",
      "\n",
      "* From 'Device Connectivity Issues', possible causes include:\n",
      "\t+ Outdated Firmware\n",
      "\t+ Limited Network Range\n",
      "\t+ Authentication Failures\n",
      "\t+ Network Driver Issues\n",
      "* From 'Routing Protocol Conflicts', potential causes include:\n",
      "\t+ Unpatched Network Device Vulnerabilities\n",
      "\t+ Poor Network Documentation\n",
      "\t+ Unmonitored Network Traffic\n",
      "\t+ Incorrect MTU Settings\n",
      "* From 'Frequent Wi-Fi Disconnections', possible causes include:\n",
      "\t+ Load Balancer Configuration Issues\n",
      "\t+ Poor VoIP Call Quality\n",
      "\t+ Misconfigured NAT Rules\n",
      "\t+ Misaligned Antennas on Wi-Fi Devices\n",
      "* From 'DHCP Lease Time Issues', potential causes include:\n",
      "\t+ Device Driver Conflicts\n",
      "\t+ Bluetooth Interference with Wi-Fi\n",
      "\t+ Duplicate Hostnames\n",
      "\t+ Unsecured SNMP Configurations\n",
      "* From 'Inconsistent IP Address Allocation', possible causes include:\n",
      "\t+ Disrupted Service from ISP\n",
      "\t+ Incorrect Firewall NAT Configuration\n",
      "\t+ Weak Encryption Standards\n",
      "\t+ Packet Duplication\n",
      "\n",
      "To resolve the router configuration issue, it's essential to investigate and troubleshoot the underlying causes. Here's a step-by-step approach:\n",
      "\n",
      "1. Check for Outdated Firmware: Ensure that the router's firmware is up-to-date by checking with the manufacturer for any available updates. Updating the firmware might resolve the connectivity issue.\n",
      "2. Verify Network Settings: Double-check that the network settings, such as IP address, subnet mask, and default gateway, are correctly configured.\n",
      "3. Check Network Range: Verify that the network range is not limited by any configuration or device constraints.\n",
      "4. Investigate Authentication Failures: Check the authentication settings and ensure that the device is configured to authenticate correctly with the network.\n",
      "5. Check Network Driver Issues: Verify that the network drivers are correctly installed and configured on the device.\n",
      "6. Check Routing Protocol Conflicts: Review the routing protocols in use and ensure that they are correctly configured and do not conflict with each other.\n",
      "7. Investigate Load Balancer Configuration Issues: Check the load balancer configuration to ensure that it is not causing issues with device connectivity.\n",
      "8. Check Poor VoIP Call Quality: Verify that the VoIP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from groq import Groq\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load your dataset from Excel\n",
    "file_path = '/content/Data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Trim whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Extract problems and solutions\n",
    "try:\n",
    "    problems = df['Problem'].tolist()\n",
    "    solutions = df['Solution'].tolist()\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e} - Please check the column names.\")\n",
    "    exit()  # Exit if there is a KeyError\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding the problems\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')  # Corrected here\n",
    "\n",
    "# Create a DataFrameLoader\n",
    "loader = DataFrameLoader(df, page_content_column='Problem')  # Use page_content_column instead of text_column\n",
    "\n",
    "# Function to perform agentic chunking\n",
    "def agentic_chunking(documents, chunk_size=300):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for doc in documents:\n",
    "        for paragraph in doc.page_content.split('\\n\\n'):  # Split by double newlines for paragraphs\n",
    "            paragraph = paragraph.strip()\n",
    "            if not paragraph:  # Skip empty paragraphs\n",
    "                continue\n",
    "\n",
    "            # Check if adding the paragraph exceeds the chunk size\n",
    "            if len(current_chunk) + len(paragraph) + 1 <= chunk_size:\n",
    "                current_chunk += f\"{paragraph}\\n\\n\"  # Add paragraph to the current chunk\n",
    "            else:\n",
    "                if current_chunk:  # If there's a current chunk, save it\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = f\"{paragraph}\\n\\n\"  # Start a new chunk with the current paragraph\n",
    "\n",
    "    # Add any remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Load documents\n",
    "documents = loader.load()\n",
    "# Apply agentic chunking\n",
    "document_chunks = agentic_chunking(documents)\n",
    "\n",
    "# Convert string chunks to Document objects\n",
    "document_chunks = [Document(page_content=chunk, metadata={}) for chunk in document_chunks]\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(document_chunks, embeddings_model)\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following problem description for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Example query to test the index\n",
    "query_text = input(\"Enter your query or problem: \")\n",
    "\n",
    "# Use Groq to refine the query\n",
    "refined_query_text = refine_query_with_groq(query_text)\n",
    "print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "# Perform the search for the top k most similar items\n",
    "retrieved_documents = vector_store.similarity_search(refined_query_text, k=5)\n",
    "\n",
    "# Combine retrieved documents as context for the generative model\n",
    "retrieval_context = \"\"\n",
    "for doc in retrieved_documents:\n",
    "    retrieval_context += f\"Problem Chunk: {doc}\\n\"  # Change this to handle string chunks\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that generates answers to user queries based on retrieved context.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context, provide a detailed solution to the query: {retrieval_context}\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",  # Assuming you're using LLaMA 3\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "# Generate the final answer with RAG\n",
    "final_answer = generate_answer_with_rag(refined_query_text, retrieval_context)\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
